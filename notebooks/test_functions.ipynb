{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python392jvsc74a57bd04cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462",
   "display_name": "Python 3.9.2 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "4cd7ab41f5fca4b9b44701077e38c5ffd31fe66a6cab21e0214b68d958d0e462"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load raw corpus\n",
    "def load_corpus(filename):\n",
    "    # open the file as read only\n",
    "\tfile = open(filename, 'r')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse & Clean Function Testing\n",
    "def parse_and_clean(input):\n",
    "    with open('../data/processed/corpus.txt', 'w') as fout:\n",
    "        text = load_corpus(input)\n",
    "        text = re.sub(r'http\\S+', '', text) # removes hyperlinks\n",
    "        text = text.replace('_', \"\") # removes underscores\n",
    "        text = text.replace('  (', \"\") # removes empty parentheses\n",
    "        text = text.replace('* ', \"\") # removes empty parentheses\n",
    "\n",
    "        fout.write(text)\n",
    "\n",
    "        tokens = text.split() # create tokens\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        tokens = [w.translate(table) for w in tokens]\n",
    "        tokens = [word for word in tokens if word.isalpha()] # remove remaining tokens that are not alphabetic\n",
    "        tokens = [word.lower() for word in tokens] # make lower case\n",
    "\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Total Tokens: 9183\nUnique Tokens: 2831\n['experimentation', 'and', 'medical', 'malpractice', 'mkultra', 'three', 'members', 'of', 'congress', 'are', 'demanding', 'answers', 'after', 'a', 'st', 'louis', 'scholars', 'new', 'book', 'revealed', 'details', 'of', 'secret', 'cold', 'warera', 'us', 'government', 'testing', 'in', 'which', 'countless', 'unsuspecting', 'people', 'including', 'many', 'children', 'pregnant', 'women', 'and', 'minorities', 'were', 'fed', 'sprayed', 'or', 'injected', 'with', 'radiation', 'and', 'other', 'dangerous', 'materials', 'st', 'louis', 'leaders', 'were', 'told', 'at', 'the', 'time', 'that', 'the', 'government', 'was', 'testing', 'a', 'smoke', 'screen', 'that', 'could', 'shield', 'the', 'city', 'from', 'aerial', 'observation', 'in', 'case', 'of', 'soviet', 'attack', 'evidence', 'now', 'shows', 'radioactive', 'material', 'not', 'just', 'zinc', 'cadmium', 'sulfide', 'was', 'part', 'of', 'that', 'spraying', 'martinotaylor', 'said', 'a', 'government', 'report', 'provides', 'for', 'the', 'first', 'time', 'a', 'comprehensive', 'official', 'history', 'of', 'britains', 'biological', 'weapons', 'trials', 'between', 'and', 'the', 'report', 'reveals', 'new', 'information', 'about', 'more', 'than', 'covert', 'experiments', 'between', 'and', 'planes', 'flew', 'from', 'northeast', 'england', 'to', 'the', 'tip', 'of', 'cornwall', 'along', 'the', 'south', 'and', 'west', 'coasts', 'dropping', 'huge', 'amounts', 'of', 'zinc', 'cadmium', 'sulphide', 'on', 'the', 'population', 'the', 'report', 'also', 'reveals', 'details', 'of', 'the', 'dice', 'trials', 'in', 'south', 'dorset', 'between', 'and', 'these', 'involved', 'us', 'and', 'uk', 'military', 'scientists', 'spraying', 'into', 'the', 'air', 'massive', 'quantities', 'of', 'serratia', 'marcescens', 'bacteria', 'with', 'an', 'anthrax', 'simulant', 'and', 'phenol', 'between', 'and', 'more', 'than', 'a', 'million', 'people', 'along', 'the']\n"
     ]
    }
   ],
   "source": [
    "tokens = parse_and_clean('/Users/logno/Documents/GitHub/conspiracy_generation/data/raw/conspiracy_theories.txt')\n",
    "print('Total Tokens: %d' % len(tokens))\n",
    "print('Unique Tokens: %d' % len(set(tokens)))\n",
    "print(tokens[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(Counter({'.': 1,\n",
       "          '/': 2,\n",
       "          't': 3,\n",
       "          's': 4,\n",
       "          'd': 5,\n",
       "          'a': 6,\n",
       "          'p': 7,\n",
       "          'r': 8,\n",
       "          'o': 9,\n",
       "          'c': 10,\n",
       "          'e': 11,\n",
       "          'u': 12,\n",
       "          'x': 13}),\n",
       " ['_', '.', '/', 't', 's', 'd', 'a', 'p', 'r', 'o', 'c', 'e', 'u', 'x'])"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "get_vocab('../data/processed/corpus.txt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}